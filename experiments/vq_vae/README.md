# Vector Quantized Variational Autoencoder (VQ-VAE)

This directory contains the implementation of a Vector Quantized Variational Autoencoder for image generation and compression.

## Directory Structure

- `src/` - Source code for the VQ-VAE implementation
- `checkpoints/` - Saved model checkpoints
- `generated_samples/` - Output samples generated by the trained model

## Model Architecture

The VQ-VAE consists of three main components:
- Encoder: Compresses input images into a continuous latent space
- Vector Quantizer: Discretizes the continuous latent space into a codebook
- Decoder: Reconstructs images from the quantized latent representations

## Training

To train the model:
1. Prepare your dataset
2. Run the training script from the `src/` directory
3. Monitor reconstruction quality and codebook usage

## Results

The model produces:
- Reconstructed images from the training set
- Generated samples using the learned codebook
- Latent space visualizations

All outputs are saved in the `generated_samples/` directory.

## Requirements

- Python 3.x
- PyTorch
- Other dependencies as specified in the project's requirements file 